"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MÃ“DULO: MODELO LSTM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PropÃ³sito: Construir y entrenar la red neuronal LSTM

Funciones:
1. build_lstm_model() â†’ Construye la arquitectura del modelo
2. train_model() â†’ Entrena el modelo con los datos
3. get_model_summary() â†’ Muestra resumen del modelo
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import os
from src.config import (
    LSTM_UNITS_1, LSTM_UNITS_2, DROPOUT_RATE,
    EPOCHS, BATCH_SIZE, MODEL_PATH, TIME_STEPS
)


def build_lstm_model(time_steps=TIME_STEPS):
    """
    Construye el modelo LSTM capa por capa
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ARQUITECTURA DEL MODELO
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Nuestra red tiene 5 capas:
    
    1. LSTM Layer 1 (50 neuronas)
       â”œâ”€ Procesa la secuencia de 60 dÃ­as
       â””â”€ Aprende patrones temporales complejos
       
    2. Dropout (20%)
       â”œâ”€ "Apaga" aleatoriamente 20% de neuronas
       â””â”€ Evita que el modelo memorice (overfitting)
       
    3. LSTM Layer 2 (50 neuronas)
       â”œâ”€ Refina los patrones aprendidos
       â””â”€ Captura relaciones mÃ¡s abstractas
       
    4. Dropout (20%)
       â””â”€ MÃ¡s regularizaciÃ³n
       
    5. Dense (1 neurona)
       â””â”€ Capa de salida: predice la temperatura
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Â¿QUÃ‰ ES Sequential?
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Sequential = modelo donde las capas se apilan una tras otra
    
    AnalogÃ­a: Como una tuberÃ­a donde el agua (datos) fluye:
    
    Entrada â†’ [Capa 1] â†’ [Capa 2] â†’ [Capa 3] â†’ Salida
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Â¿POR QUÃ‰ 2 CAPAS LSTM?
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Una sola capa LSTM:
    - Aprende patrones simples
    - Ejemplo: "Si sube 3 dÃ­as â†’ sigue subiendo"
    
    Dos capas LSTM (stacked):
    - Primera capa: Aprende patrones bÃ¡sicos
    - Segunda capa: Aprende patrones de patrones
    - Ejemplo: "Detecta ciclos semanales dentro de tendencias mensuales"
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Args:
        time_steps: Longitud de la secuencia de entrada (60 dÃ­as)
    
    Returns:
        model: Modelo LSTM compilado y listo para entrenar
    """
    
    # Inicializar el modelo secuencial
    # Sequential = capas en secuencia (una tras otra)
    model = Sequential()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CAPA 1: Primera LSTM
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    model.add(LSTM(
        units=LSTM_UNITS_1,           # 50 neuronas LSTM
        return_sequences=True,        # â† IMPORTANTE: explicaciÃ³n abajo
        input_shape=(time_steps, 1)   # (60 dÃ­as, 1 caracterÃ­stica)
    ))
    
    """
    Â¿QUÃ‰ ES return_sequences=True?
    
    Controla QUÃ‰ devuelve la capa LSTM:
    
    return_sequences=False (default):
    - Solo devuelve el OUTPUT del Ãºltimo paso
    - Salida: (batch_size, units)
    - Ejemplo: (32, 50) â†’ 32 muestras, 50 valores
    
    return_sequences=True:
    - Devuelve TODOS los outputs de cada paso
    - Salida: (batch_size, time_steps, units)
    - Ejemplo: (32, 60, 50) â†’ 32 muestras, 60 pasos, 50 valores por paso
    
    Â¿CUÃNDO usar True?
    - Cuando hay OTRA capa LSTM despuÃ©s
    - La siguiente capa LSTM necesita la secuencia completa
    
    AnalogÃ­a:
    - False = "Dame solo el resumen final del libro"
    - True = "Dame el resumen de cada capÃ­tulo"
    
    En nuestro caso:
    Primera LSTM â†’ return_sequences=True â†’ pasa 60 salidas a segunda LSTM
    Segunda LSTM â†’ return_sequences=False â†’ da solo la predicciÃ³n final
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CAPA 2: Dropout (RegularizaciÃ³n)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    model.add(Dropout(DROPOUT_RATE))  # 0.2 = 20%
    
    """
    Â¿QUÃ‰ HACE DROPOUT?
    
    Durante el ENTRENAMIENTO:
    - Apaga aleatoriamente el 20% de las neuronas
    - Cada Ã©poca apaga neuronas DIFERENTES
    
    Ejemplo visual:
    Ã‰poca 1: âš«âš«âšªâš«âš«âšªâš«âš«âšªâš«  (âšª = apagada)
    Ã‰poca 2: âš«âšªâš«âš«âšªâš«âš«âšªâš«âš«  (diferentes neuronas)
    
    Durante la PREDICCIÃ“N:
    - TODAS las neuronas activas (0% dropout)
    
    Â¿POR QUÃ‰ ES ÃšTIL?
    
    Sin Dropout (puede memorizar):
    - Neurona 1: "Si dÃ­a 27 = 15Â°C â†’ dÃ­a 28 = 16Â°C"
    - Memoriza ejemplos especÃ­ficos âŒ
    - En datos nuevos: falla
    
    Con Dropout (aprende robusto):
    - Cada neurona debe ser Ãºtil por sÃ­ sola
    - No puede depender de otras neuronas especÃ­ficas
    - Aprende patrones generales âœ…
    - En datos nuevos: funciona bien
    
    AnalogÃ­a:
    Estudiando para un examen:
    - Sin Dropout: Memorizas respuestas exactas
    - Con Dropout: Entiendes los conceptos
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CAPA 3: Segunda LSTM
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    model.add(LSTM(
        units=LSTM_UNITS_2,        # 50 neuronas LSTM
        return_sequences=False     # â† Solo output final
    ))
    
    """
    Â¿POR QUÃ‰ return_sequences=False aquÃ­?
    
    Es la ÃšLTIMA capa LSTM:
    - Ya no hay mÃ¡s capas LSTM despuÃ©s
    - Solo necesitamos UNA predicciÃ³n final
    - No necesitamos la secuencia completa
    
    Salida de esta capa: (batch_size, 50)
    Ejemplo: (32, 50) â†’ 32 muestras, 50 valores cada una
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CAPA 4: Dropout (Segunda regularizaciÃ³n)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    model.add(Dropout(DROPOUT_RATE))  # Otra vez 20%
    
    """
    Â¿POR QUÃ‰ otro Dropout?
    
    MÃ¡s capas de Dropout = mÃ¡s regularizaciÃ³n
    - Evita aÃºn mÃ¡s el overfitting
    - Especialmente Ãºtil con redes profundas (2+ capas)
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CAPA 5: Dense (Capa de Salida)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    model.add(Dense(units=1))
    
    """
    Â¿QUÃ‰ ES Dense?
    
    Dense = Capa totalmente conectada (Fully Connected)
    - Cada neurona conectada a TODAS las del paso anterior
    
    Â¿POR QUÃ‰ units=1?
    
    Queremos predecir UN solo valor: la temperatura de maÃ±ana
    
    Si quisiÃ©ramos predecir los prÃ³ximos 7 dÃ­as:
    - units=7 (una salida por cada dÃ­a)
    
    Entrada a Dense: 50 valores (de la LSTM anterior)
    Salida de Dense: 1 valor (temperatura predicha)
    
    CÃ¡lculo interno:
    output = suma(50_valores Ã— 50_pesos) + bias
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # COMPILAR EL MODELO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    model.compile(
        optimizer='adam',              # Algoritmo de optimizaciÃ³n
        loss='mean_squared_error'      # FunciÃ³n de pÃ©rdida
    )
    
    """
    Â¿QUÃ‰ ES COMPILAR?
    
    Compilar = Configurar cÃ³mo el modelo va a aprender
    
    Necesitas especificar:
    1. Optimizer (optimizador)
    2. Loss (funciÃ³n de pÃ©rdida)
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. OPTIMIZER: 'adam'
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Â¿QuÃ© hace?
    - Ajusta los pesos del modelo para reducir el error
    
    Â¿CÃ³mo funciona?
    Piensa en bajar una montaÃ±a con los ojos vendados:
    
    SGD (optimizador simple):
    - Das pasos del mismo tamaÃ±o siempre
    - Puedes pasarte el valle o tardar mucho
    
    Adam (optimizador inteligente):
    - Ajusta el tamaÃ±o del paso automÃ¡ticamente
    - Pasos grandes cuando estÃ¡s lejos del Ã³ptimo
    - Pasos pequeÃ±os cuando estÃ¡s cerca
    - Recuerda la direcciÃ³n de pasos anteriores
    
    Â¿Por quÃ© Adam?
    âœ… Funciona bien en la mayorÃ­a de casos
    âœ… Converge mÃ¡s rÃ¡pido
    âœ… No necesitas ajustar mucho el learning rate
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    2. LOSS: 'mean_squared_error' (MSE)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Â¿QuÃ© es?
    - Una funciÃ³n que mide "quÃ© tan mal" predecimos
    
    FÃ³rmula:
    MSE = promedio((predicciÃ³n - real)Â²)
    
    Ejemplo:
    Predicciones: [20, 21, 19]
    Reales:       [21, 22, 20]
    Errores:      [-1, -1, -1]
    ErroresÂ²:     [1,  1,  1]
    MSE = (1 + 1 + 1) / 3 = 1.0
    
    Â¿Por quÃ© elevar al cuadrado?
    - Penaliza mÃ¡s los errores grandes
    - Error de 2Â°C pesa 4 veces mÃ¡s que error de 1Â°C
    - Error de 10Â°C pesa 100 veces mÃ¡s que error de 1Â°C
    
    Â¿Por quÃ© MSE para temperaturas?
    âœ… Penaliza errores grandes (importante en clima)
    âœ… FunciÃ³n suave (buena para gradientes)
    âœ… EstÃ¡ndar para problemas de regresiÃ³n
    
    Otras opciones (no las usamos):
    - MAE: Mean Absolute Error (penaliza igual todos los errores)
    - Huber: HÃ­brido entre MSE y MAE
    """
    
    # Mostrar informaciÃ³n
    print("âœ… Modelo LSTM creado")
    print(f"\n   ğŸ“ Arquitectura:")
    print(f"      1. LSTM Layer 1    : {LSTM_UNITS_1} unidades (return_sequences=True)")
    print(f"      2. Dropout         : {DROPOUT_RATE*100}%")
    print(f"      3. LSTM Layer 2    : {LSTM_UNITS_2} unidades (return_sequences=False)")
    print(f"      4. Dropout         : {DROPOUT_RATE*100}%")
    print(f"      5. Dense (Output)  : 1 unidad")
    print(f"\n   âš™ï¸  ConfiguraciÃ³n:")
    print(f"      Optimizer: Adam")
    print(f"      Loss: Mean Squared Error (MSE)")
    
    return model


def train_model(model, X_train, y_train, X_val, y_val):
    """
    Entrena el modelo LSTM con los datos
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Â¿QUÃ‰ PASA DURANTE EL ENTRENAMIENTO?
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    El entrenamiento es un proceso iterativo:
    
    CADA Ã‰POCA (epoch):
    â”œâ”€ 1. Forward Pass (hacia adelante):
    â”‚     - El modelo ve los datos
    â”‚     - Hace predicciones
    â”‚     - Ejemplo: predice [20.5, 21.3, 19.8...]
    â”‚
    â”œâ”€ 2. Calcular Loss (pÃ©rdida):
    â”‚     - Compara predicciones vs realidad
    â”‚     - Calcula MSE
    â”‚     - Ejemplo: MSE = 2.5Â°C
    â”‚
    â”œâ”€ 3. Backward Pass (hacia atrÃ¡s):
    â”‚     - Calcula gradientes (derivadas)
    â”‚     - Determina cÃ³mo ajustar cada peso
    â”‚     - "Si bajo este peso 0.01, el error baja 0.1"
    â”‚
    â”œâ”€ 4. Actualizar Pesos:
    â”‚     - Adam optimizer ajusta todos los pesos
    â”‚     - Las 3 puertas LSTM aprenden
    â”‚     - Dense layer aprende
    â”‚
    â””â”€ 5. ValidaciÃ³n:
          - EvalÃºa en datos de validaciÃ³n
          - Si no mejora â†’ EarlyStopping considera parar
    
    REPITE esto EPOCHS veces (50 por default)
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    CALLBACKS: Funciones que se ejecutan durante entrenamiento
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Son como "asistentes" que vigilan el entrenamiento
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Args:
        model: Modelo LSTM construido
        X_train, y_train: Datos de entrenamiento
        X_val, y_val: Datos de validaciÃ³n
    
    Returns:
        history: Objeto con historial del entrenamiento
                 (pÃ©rdidas por Ã©poca, mÃ©tricas, etc.)
    """
    
    # Crear carpeta models si no existe
    os.makedirs('models', exist_ok=True)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CALLBACK 1: EarlyStopping
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    early_stop = EarlyStopping(
        monitor='val_loss',           # Vigilar la pÃ©rdida de validaciÃ³n
        patience=10,                  # Esperar 10 Ã©pocas sin mejora
        restore_best_weights=True,    # Restaurar mejores pesos
        verbose=1                     # Mostrar mensajes
    )
    
    """
    Â¿QUÃ‰ HACE EarlyStopping?
    
    Detiene el entrenamiento si deja de mejorar
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    PARÃMETROS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    monitor='val_loss':
    - Vigila la pÃ©rdida en datos de VALIDACIÃ“N
    - No la pÃ©rdida de entrenamiento (esa siempre baja)
    
    Â¿Por quÃ© validaciÃ³n?
    - Si val_loss baja â†’ modelo mejora en datos nuevos âœ…
    - Si val_loss sube pero train_loss baja â†’ overfitting âŒ
    
    patience=10:
    - "Dame 10 oportunidades para mejorar"
    - Si en 10 Ã©pocas no hay mejora â†’ STOP
    
    Ejemplo:
    Ã‰poca 20: val_loss = 2.5 (mejor hasta ahora)
    Ã‰poca 21: val_loss = 2.6 (peor, contador = 1)
    Ã‰poca 22: val_loss = 2.7 (peor, contador = 2)
    ...
    Ã‰poca 30: val_loss = 2.8 (contador = 10) â†’ Â¡STOP!
    
    restore_best_weights=True:
    - Al parar, restaura los pesos de la mejor Ã©poca
    - No te quedas con los pesos finales (que pueden ser peores)
    
    Ejemplo:
    Ã‰poca 20: val_loss = 2.5 â† MEJOR
    Ã‰poca 30: val_loss = 2.8 â† Final (peor)
    EarlyStopping restaura pesos de Ã©poca 20 âœ…
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Â¿POR QUÃ‰ ES IMPORTANTE?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Sin EarlyStopping:
    - Entrenas 50 Ã©pocas completas
    - Puede que en Ã©poca 25 ya era Ã³ptimo
    - Ã‰pocas 26-50 = desperdicio de tiempo
    - Peor: puede sobreajustarse (overfitting)
    
    Con EarlyStopping:
    - Para automÃ¡ticamente cuando es Ã³ptimo
    - Ahorra tiempo â±ï¸
    - Evita overfitting âœ…
    - Mejores resultados ğŸ“ˆ
    
    AnalogÃ­a:
    Como estudiar para un examen:
    - Sin EarlyStopping: Estudias hasta las 3am aunque ya lo sepas
    - Con EarlyStopping: Paras cuando ya dominas el tema
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CALLBACK 2: ModelCheckpoint
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    checkpoint = ModelCheckpoint(
        MODEL_PATH,                   # Ruta donde guardar
        monitor='val_loss',           # Vigilar val_loss
        save_best_only=True,          # Solo guardar si mejora
        verbose=1                     # Mostrar mensajes
    )
    
    """
    Â¿QUÃ‰ HACE ModelCheckpoint?
    
    Guarda el modelo en disco cuando mejora
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    PARÃMETROS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    save_best_only=True:
    - Solo guarda cuando val_loss mejora
    - No sobreescribe con versiones peores
    
    Ejemplo:
    Ã‰poca 10: val_loss = 3.0 â†’ GUARDAR âœ…
    Ã‰poca 11: val_loss = 3.2 â†’ No guardar
    Ã‰poca 12: val_loss = 2.8 â†’ GUARDAR âœ… (mejor que 3.0)
    Ã‰poca 13: val_loss = 2.9 â†’ No guardar
    
    Resultado final: Modelo guardado con val_loss = 2.8
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Â¿POR QUÃ‰ ES ÃšTIL?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Seguridad:
    - Si el entrenamiento se interrumpe (corte de luz, etc.)
    - Ya tienes el mejor modelo guardado
    
    ComparaciÃ³n:
    - Puedes entrenar mÃºltiples veces
    - Comparar cuÃ¡l modelo fue mejor
    
    Reproducibilidad:
    - El modelo guardado puede usarse despuÃ©s
    - Sin necesidad de re-entrenar
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # INICIAR ENTRENAMIENTO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print("\nğŸš€ Iniciando entrenamiento...")
    print("="*70)
    print(f"   ğŸ“Š ConfiguraciÃ³n:")
    print(f"      Ã‰pocas mÃ¡ximas    : {EPOCHS}")
    print(f"      Batch size        : {BATCH_SIZE}")
    print(f"      Datos training    : {len(X_train)} muestras")
    print(f"      Datos validation  : {len(X_val)} muestras")
    print(f"\n   ğŸ¯ Callbacks:")
    print(f"      EarlyStopping     : Paciencia de 10 Ã©pocas")
    print(f"      ModelCheckpoint   : Guarda mejor modelo")
    print("="*70 + "\n")
    
    """
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    EXPLICACIÃ“N DE PARÃMETROS DE model.fit()
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    
    history = model.fit(
        X_train, y_train,                    # Datos de entrenamiento
        epochs=EPOCHS,                       # MÃ¡ximo de Ã©pocas
        batch_size=BATCH_SIZE,              # TamaÃ±o de lote
        validation_data=(X_val, y_val),     # Datos de validaciÃ³n
        callbacks=[early_stop, checkpoint],  # Callbacks
        verbose=1                            # Mostrar progreso
    )
    
    """
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    PARÃMETRO: batch_size=32
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Â¿QuÃ© es un batch (lote)?
    
    No procesamos TODAS las muestras a la vez
    Las dividimos en grupos pequeÃ±os (batches)
    
    Ejemplo con 3200 muestras y batch_size=32:
    - Batch 1: Muestras 1-32   â†’ Procesa â†’ Actualiza pesos
    - Batch 2: Muestras 33-64  â†’ Procesa â†’ Actualiza pesos
    - Batch 3: Muestras 65-96  â†’ Procesa â†’ Actualiza pesos
    - ...
    - Batch 100: Muestras 3169-3200 â†’ Procesa â†’ Actualiza pesos
    
    1 Ã‰POCA = Procesar todos los batches una vez
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Â¿POR QUÃ‰ BATCHES?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Ventaja 1: MEMORIA
    - No cabe todo en RAM/GPU a la vez
    - Batches pequeÃ±os â†’ menos memoria
    
    Ventaja 2: VELOCIDAD
    - GPU procesa batches en paralelo
    - MÃ¡s eficiente que uno por uno
    
    Ventaja 3: MEJOR APRENDIZAJE
    - Actualizar pesos frecuentemente (cada batch)
    - Converge mÃ¡s rÃ¡pido
    - Mejor generalizaciÃ³n
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    COMPARACIÃ“N DE TAMAÃ‘OS DE BATCH:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    batch_size=1 (Stochastic Gradient Descent):
    âŒ Muy lento
    âŒ Actualizaciones muy ruidosas
    âœ… Puede escapar de mÃ­nimos locales
    
    batch_size=32 (RECOMENDADO):
    âœ… Buen balance velocidad/memoria
    âœ… Actualizaciones estables
    âœ… Funciona bien en la mayorÃ­a de casos
    
    batch_size=3200 (Batch completo):
    âŒ Requiere mucha memoria
    âœ… Actualizaciones muy precisas
    âŒ Puede quedar atrapado en mÃ­nimos locales
    âŒ MÃ¡s lento
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    PARÃMETRO: verbose=1
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Controla cuÃ¡nta informaciÃ³n mostrar:
    
    verbose=0: Silencioso (nada)
    verbose=1: Barra de progreso + mÃ©tricas (RECOMENDADO)
    verbose=2: Una lÃ­nea por Ã©poca (sin barra)
    
    Ejemplo de salida con verbose=1:
    Epoch 1/50
    100/100 [==============================] - 5s - loss: 0.0234 - val_loss: 0.0198
    """
    
    print("\nâœ… Entrenamiento completado")
    print(f"   Total de Ã©pocas ejecutadas: {len(history.history['loss'])}")
    print(f"   Mejor val_loss: {min(history.history['val_loss']):.6f}")
    
    return history


def get_model_summary(model):
    """
    Muestra un resumen detallado del modelo
    
    Incluye:
    - Arquitectura de capas
    - ParÃ¡metros (pesos) por capa
    - Total de parÃ¡metros entrenables
    
    Args:
        model: Modelo LSTM
    """
    print("\n" + "="*70)
    print("ğŸ“‹ RESUMEN DEL MODELO")
    print("="*70)
    model.summary()
    print("="*70)
    
    """
    Â¿QUÃ‰ MUESTRA model.summary()?
    
    Ejemplo de salida:
    
    Layer (type)                 Output Shape              Param #   
    =================================================================
    lstm (LSTM)                  (None, 60, 50)            10400     
    dropout (Dropout)            (None, 60, 50)            0         
    lstm_1 (LSTM)                (None, 50)                20200     
    dropout_1 (Dropout)          (None, 50)                0         
    dense (Dense)                (None, 1)                 51        
    =================================================================
    Total params: 30,651
    Trainable params: 30,651
    Non-trainable params: 0
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    EXPLICACIÃ“N:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Output Shape:
    - (None, 60, 50) = (batch_size, time_steps, units)
    - None = puede variar segÃºn el batch
    
    Param # (ParÃ¡metros):
    - Cantidad de pesos que aprende esa capa
    
    Â¿Por quÃ© LSTM tiene 10,400 parÃ¡metros?
    
    LSTM tiene 4 sets de pesos (3 puertas + candidato):
    - Forget Gate: pesos
    - Input Gate: pesos  
    - Output Gate: pesos
    - Candidate: pesos
    
    FÃ³rmula para LSTM:
    params = 4 Ã— (units Ã— (units + input_dim + 1))
    params = 4 Ã— (50 Ã— (50 + 1 + 1))
    params = 4 Ã— (50 Ã— 52)
    params = 10,400 âœ…
    
    Â¿Por quÃ© Dense tiene 51 parÃ¡metros?
    params = (input_size Ã— output_size) + bias
    params = (50 Ã— 1) + 1
    params = 51 âœ…
    
    Â¿Por quÃ© Dropout tiene 0 parÃ¡metros?
    - Dropout NO tiene pesos
    - Solo apaga neuronas aleatoriamente
    - No aprende nada
    """


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE DE PRUEBA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸ§ª PROBANDO MÃ“DULO DEL MODELO")
    print("="*70 + "\n")
    
    # Construir modelo
    model = build_lstm_model()
    
    # Mostrar resumen
    get_model_summary(model)
    
    print("\nğŸ’¡ Nota: Para entrenar el modelo, usa el script train.py")
    print("="*70)