"""
M√≥dulo para entrenamiento de modelos
"""
import sys
from pathlib import Path

# Agregar la ra√≠z del proyecto al PATH
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)
import joblib
from config.config import MODELS_DIR, RANDOM_STATE


def train_logistic_regression(X_train, y_train, X_val, y_val):
    """
    Entrena un modelo de Regresi√≥n Log√≠stica
    
    Logistic Regression: Modelo lineal que predice probabilidades
    - Simple y r√°pido
    - Buena l√≠nea base
    - Funciona bien cuando las clases son linealmente separables
    
    Args:
        X_train: Features de entrenamiento
        y_train: Target de entrenamiento
        X_val: Features de validaci√≥n
        y_val: Target de validaci√≥n
        
    Returns:
        tuple: (modelo entrenado, m√©tricas en validaci√≥n)
    """
    print("\n" + "="*60)
    print("üîµ ENTRENANDO LOGISTIC REGRESSION")
    print("="*60)
    
    # Crear y entrenar modelo
    model = LogisticRegression(
        random_state=RANDOM_STATE,
        max_iter=1000,
        class_weight='balanced'  # Dar m√°s peso a la clase minoritaria
    )
    
    print("‚è≥ Entrenando modelo...")
    model.fit(X_train, y_train)
    print("‚úÖ Modelo entrenado")
    
    # Evaluar en validaci√≥n
    y_pred = model.predict(X_val)
    y_pred_proba = model.predict_proba(X_val)[:, 1]
    
    # Calcular m√©tricas
    metrics = calculate_metrics(y_val, y_pred, y_pred_proba, "Logistic Regression")
    
    return model, metrics


def train_random_forest(X_train, y_train, X_val, y_val):
    """
    Entrena un modelo de Random Forest
    
    Random Forest: Ensemble de muchos √°rboles de decisi√≥n
    - M√°s robusto que un solo √°rbol
    - Maneja bien interacciones no lineales
    - Resistente al overfitting
    
    Args:
        X_train: Features de entrenamiento
        y_train: Target de entrenamiento
        X_val: Features de validaci√≥n
        y_val: Target de validaci√≥n
        
    Returns:
        tuple: (modelo entrenado, m√©tricas en validaci√≥n)
    """
    print("\n" + "="*60)
    print("üå≤ ENTRENANDO RANDOM FOREST")
    print("="*60)
    
    # Crear y entrenar modelo
    model = RandomForestClassifier(
        n_estimators=100,  # N√∫mero de √°rboles
        max_depth=10,  # Profundidad m√°xima de cada √°rbol
        random_state=RANDOM_STATE,
        class_weight='balanced',
        n_jobs=-1  # Usar todos los cores del CPU
    )
    
    print("‚è≥ Entrenando modelo (esto puede tardar 1-2 minutos)...")
    model.fit(X_train, y_train)
    print("‚úÖ Modelo entrenado")
    
    # Evaluar en validaci√≥n
    y_pred = model.predict(X_val)
    y_pred_proba = model.predict_proba(X_val)[:, 1]
    
    # Calcular m√©tricas
    metrics = calculate_metrics(y_val, y_pred, y_pred_proba, "Random Forest")
    
    return model, metrics


def calculate_metrics(y_true, y_pred, y_pred_proba, model_name):
    """
    Calcula m√©tricas de evaluaci√≥n para clasificaci√≥n
    
    Args:
        y_true: Valores reales
        y_pred: Predicciones del modelo
        y_pred_proba: Probabilidades predichas
        model_name: Nombre del modelo
        
    Returns:
        dict: Diccionario con todas las m√©tricas
    """
    print(f"\nüìä M√âTRICAS DE EVALUACI√ìN - {model_name}")
    print("="*60)
    
    # Calcular m√©tricas
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    
    print(f"\nüéØ M√©tricas Principales:")
    print(f"   ‚Ä¢ Precision: {precision:.4f} - De los que predice como fraude, {precision*100:.2f}% son correctos")
    print(f"   ‚Ä¢ Recall:    {recall:.4f} - Detecta {recall*100:.2f}% de todos los fraudes")
    print(f"   ‚Ä¢ F1-Score:  {f1:.4f} - Balance entre Precision y Recall")
    print(f"   ‚Ä¢ ROC-AUC:   {roc_auc:.4f} - Capacidad de distinguir entre clases")
    
    # Matriz de confusi√≥n
    cm = confusion_matrix(y_true, y_pred)
    print(f"\nüìã Matriz de Confusi√≥n:")
    print(f"                 Predicho")
    print(f"               Leg√≠tima  Fraude")
    print(f"Real Leg√≠tima    {cm[0,0]:6d}  {cm[0,1]:6d}")
    print(f"     Fraude      {cm[1,0]:6d}  {cm[1,1]:6d}")
    
    print(f"\nüîç Interpretaci√≥n:")
    print(f"   ‚Ä¢ Verdaderos Negativos (TN): {cm[0,0]:,} - Leg√≠timas correctamente identificadas")
    print(f"   ‚Ä¢ Falsos Positivos (FP):     {cm[0,1]:,} - Leg√≠timas marcadas como fraude")
    print(f"   ‚Ä¢ Falsos Negativos (FN):     {cm[1,0]:,} - Fraudes NO detectados ‚ö†Ô∏è")
    print(f"   ‚Ä¢ Verdaderos Positivos (TP): {cm[1,1]:,} - Fraudes correctamente detectados ‚úÖ")
    
    metrics = {
        'model_name': model_name,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc,
        'confusion_matrix': cm
    }
    
    return metrics


def save_model(model, model_name):
    """
    Guarda el modelo entrenado
    
    Args:
        model: Modelo entrenado
        model_name: Nombre del modelo
    """
    model_path = MODELS_DIR / f"{model_name.lower().replace(' ', '_')}.pkl"
    joblib.dump(model, model_path)
    print(f"\nüíæ Modelo guardado en: {model_path}")


def compare_models(metrics_list):
    """
    Compara los resultados de m√∫ltiples modelos
    
    Args:
        metrics_list: Lista de diccionarios con m√©tricas de cada modelo
    """
    print("\n" + "="*60)
    print("üìä COMPARACI√ìN DE MODELOS")
    print("="*60)
    
    # Crear DataFrame para comparar
    comparison = pd.DataFrame([
        {
            'Modelo': m['model_name'],
            'Precision': f"{m['precision']:.4f}",
            'Recall': f"{m['recall']:.4f}",
            'F1-Score': f"{m['f1_score']:.4f}",
            'ROC-AUC': f"{m['roc_auc']:.4f}"
        }
        for m in metrics_list
    ])
    
    print("\n", comparison.to_string(index=False))
    
    # Determinar el mejor modelo (por F1-Score)
    best_model_idx = np.argmax([m['f1_score'] for m in metrics_list])
    best_model = metrics_list[best_model_idx]['model_name']
    
    print(f"\nüèÜ MEJOR MODELO: {best_model}")
    print(f"   (Basado en F1-Score)")


if __name__ == "__main__":
    # Prueba del m√≥dulo
    from src.data.load import load_data
    from src.data.preprocess import preprocess_pipeline
    
    print("üöÄ Iniciando entrenamiento de modelos...")
    
    # 1. Cargar y preprocesar datos
    data = load_data()
    if data is not None:
        processed_data = preprocess_pipeline(data, apply_balancing=True)
        
        # Extraer conjuntos de datos
        X_train = processed_data['X_train']
        y_train = processed_data['y_train']
        X_val = processed_data['X_val']
        y_val = processed_data['y_val']
        
        # 2. Entrenar modelos
        all_metrics = []
        all_models = []
        
        # Logistic Regression
        lr_model, lr_metrics = train_logistic_regression(X_train, y_train, X_val, y_val)
        save_model(lr_model, "logistic_regression")
        all_metrics.append(lr_metrics)
        all_models.append(('Logistic Regression', lr_model))
        
        # Random Forest
        rf_model, rf_metrics = train_random_forest(X_train, y_train, X_val, y_val)
        save_model(rf_model, "random_forest")
        all_metrics.append(rf_metrics)
        all_models.append(('Random Forest', rf_model))
        
        # 3. Comparar modelos
        compare_models(all_metrics)